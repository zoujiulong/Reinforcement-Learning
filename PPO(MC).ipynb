{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMht/fmk/7UDsrz8vb7NPi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoujiulong/Reinforcement-Learning/blob/main/PPO(MC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9zBL3RSkWFM"
      },
      "outputs": [],
      "source": [
        "from abc import abstractproperty\n",
        "from gym.wrappers import TimeLimit\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "EPISODES = 1000\n",
        "GAMMA = 0.99\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 8\n",
        "eps=0.2\n",
        "# Define the Q-Network\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "      super().__init__()\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(state_dim, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, action_dim)\n",
        "      )\n",
        "  def forward(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dim):\n",
        "      super().__init__()\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(state_dim, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 1),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.fc(x)\n",
        "\n",
        "def update(old_net,new_net):\n",
        "  old_net.load_state_dict(new_net.state_dict())\n",
        "\n",
        "def to_tensor(x):\n",
        "  return torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env = TimeLimit(env, max_episode_steps=500)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    print('state_dim',state_dim)\n",
        "    action_dim = env.action_space.n\n",
        "    # 新旧策略\n",
        "    actor = Actor(state_dim, action_dim)\n",
        "    old_actor= Actor(state_dim, action_dim)\n",
        "    update(old_actor,actor)\n",
        "    critic = Critic(state_dim)\n",
        "    actor_opt = Adam(actor.parameters(), lr=LR)\n",
        "    critic_opt = Adam(critic.parameters(), lr=LR)\n",
        "    trajectory_batch=[]\n",
        "    epoch=list(range(EPISODES))\n",
        "    reward_l=[]\n",
        "    for episode in tqdm(range(EPISODES), desc=\"Training\"):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        trajectory=[]\n",
        "        # 收集数据\n",
        "        while not done:\n",
        "          state_tensor = to_tensor(state).unsqueeze(0)\n",
        "          logits = old_actor(state_tensor)\n",
        "          dist = Categorical(logits=logits)\n",
        "          action = dist.sample()\n",
        "          log_prob = dist.log_prob(action)\n",
        "          # old_p=torch.exp(log_prob)\n",
        "          next_state, reward, done, _ = env.step(action.item())\n",
        "          trajectory.append([state_tensor, action, reward, next_state, log_prob])\n",
        "          state = next_state\n",
        "          total_reward += reward\n",
        "        trajectory_batch.append(trajectory)\n",
        "        # 更新\n",
        "        if (episode+1)%10==0:\n",
        "          for traj in trajectory_batch:\n",
        "            states_tensor=torch.stack([step[0] for step in traj])\n",
        "            actions_tensor=torch.stack([step[1] for step in traj] )\n",
        "            rewards_tensor=to_tensor(np.array([step[2] for step in traj]) ).unsqueeze(1)\n",
        "            G=0\n",
        "            returns=[]\n",
        "            for r in reversed(rewards_tensor):\n",
        "              G=r+GAMMA*G\n",
        "              # print(G.shape)\n",
        "              returns.insert(0,G)\n",
        "            returns_tensor=to_tensor(returns).unsqueeze(1)\n",
        "            # print(returns_tensor.shape)\n",
        "\n",
        "            next_states=to_tensor(np.array([step[3] for step in traj]) )\n",
        "\n",
        "            old_p_tensor=torch.stack([step[4] for step in traj]).unsqueeze(1)\n",
        "\n",
        "            V_s=critic(states_tensor)\n",
        "            # with torch.no_grad():\n",
        "            #   V_s1=critic(next_states)\n",
        "            #   td_target = rewards_tensor + GAMMA * V_s1*(1-dones_tensor)\n",
        "\n",
        "\n",
        "            # advantage=(td_target-V_s).detach()\n",
        "\n",
        "\n",
        "            # 概率之比\n",
        "            advantage=(returns_tensor-V_s).detach()\n",
        "            # advantage=(advantage-advantage.mean())/(advantage.std()+1e-8)\n",
        "            logits = actor(states_tensor)\n",
        "            dist = Categorical(logits=logits)\n",
        "            log_probs = dist.log_prob(actions_tensor).unsqueeze(1)\n",
        "            ratios=(log_probs-old_p_tensor.detach()).exp()\n",
        "            # 约束\n",
        "            adv=torch.min(ratios*advantage,torch.clamp(ratios,1-eps,1+eps)*advantage)\n",
        "            # adv=torch.min(ratios*advantage,torch.clamp(ratios,1-eps,1+eps)*advantage).detach()\n",
        "            # actor_loss=(-log_probs*adv).mean()\n",
        "            actor_loss=-adv.mean()\n",
        "\n",
        "            critic_loss=nn.MSELoss()(V_s.squeeze(-1),returns_tensor)\n",
        "            # critic_loss=nn.MSELoss()(V_s.squeeze(-1),td_target)\n",
        "            critic_opt.zero_grad()\n",
        "            actor_opt.zero_grad()\n",
        "            (critic_loss+actor_loss).backward()\n",
        "            critic_opt.step()\n",
        "            actor_opt.step()\n",
        "          trajectory_batch.clear()\n",
        "        update(old_actor,actor)\n",
        "        reward_l.append(total_reward)\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}, Total reward: {total_reward:.1f}\")\n",
        "    plt.plot(epoch,reward_l,label='reward')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # torch.autograd.set_detect_anomaly(True)\n",
        "  train()"
      ]
    }
  ]
}